# tag::adocKafka[]
quarkus.http.port=${PORT:8085}
quarkus.http.host=0.0.0.0

## Kafka configuration
mp.messaging.incoming.fights.connector=smallrye-kafka
mp.messaging.incoming.fights.value.deserializer=io.quarkus.workshop.superheroes.statistics.FightDeserializer
mp.messaging.incoming.fights.auto.offset.reset=earliest
# end::adocKafka[]

## Google Cloud configuration (Cloud Run + Cloud SQL)

%googlecloud.quarkus.log.console.level=INFO
%googlecloud.quarkus.log.console.color=false

# Required connection configs for Kafka producer, consumer, and admin
%googlecloud.mp.messaging.incoming.fights.bootstrap.servers=${KAFKA_CLUSTER_BOOTSTRAP_SERVERS}
%googlecloud.mp.messaging.incoming.fights.security.protocol=SASL_SSL
%googlecloud.mp.messaging.incoming.fights.sasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule required username="${KAFKA_CLUSTER_API_KEY}" password="${KAFKA_CLUSTER_API_SECRET}";
%googlecloud.mp.messaging.incoming.fights.sasl.mechanism=PLAIN
# Required for correctness in Apache Kafka clients prior to 2.6
%googlecloud.mp.messaging.incoming.fights.client.dns.lookup=use_all_dns_ips
# Best practice for Kafka producer to prevent data loss
%googlecloud.mp.messaging.incoming.fights.acks=all
## Required connection configs for Confluent Cloud Schema Registry
#schema.registry.url=https://{{ SR_ENDPOINT }}
#basic.auth.credentials.source=USER_INFO
#basic.auth.user.info={{ SR_API_KEY }}:{{ SR_API_SECRET }}
